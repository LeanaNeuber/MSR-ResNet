{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ResNet-MSRAction3D",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iv36roLuUbqX"
      },
      "source": [
        "# Implementing a ResNet with the MSR Action 3D Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CodT6558Uhl0"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lHUEPJxez3D"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDj10Rw0Pqmp"
      },
      "source": [
        "# Needed imports\n",
        "import sys\n",
        "import math    \n",
        "import os\n",
        "from os.path import join\n",
        "from os import listdir\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "from skimage import exposure\n",
        "\n",
        "import keras\n",
        "from keras.layers import Dense, Conv2D, BatchNormalization, Activation, Dropout, GlobalAveragePooling2D, Input, Flatten\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "from keras.optimizers import SGD\n",
        "from keras.models import Model\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.applications import ResNet50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FojLpNIUlHt"
      },
      "source": [
        "## Importing and Preprocessing the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KH2iXJqLPxyK"
      },
      "source": [
        "# 557 sequences\n",
        "# three subsets: AS1, AS2, AS3\n",
        "# In each category: 5 Actors for training, rest for testing\n",
        "# Each data input: (joints, frames, coords)\n",
        "# All coordinates are normalized\n",
        "# Resize to 40x40\n",
        "# Rearrange pixels: P1 --> P2 --> P3 --> P4 --> P5,\n",
        "# so Joints: (8,9,19,11,23) --> (4,5,6,7,21) --> (3,2,20,1,0) --> (16,17,18,19) --> (12,13,14,15)\n",
        "trainSubjects = [1,3,5,7,9]\n",
        "testSubjects = [2,4,6,8,10]\n",
        "groupAS1 = [2,3,5,6,10,13,18,20]\n",
        "groupAS2 = [1,4,7,8,9,11,14,12]\n",
        "groupAS3 = [6,14,15,16,17,18,19,20]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JY99jBzgP5-j"
      },
      "source": [
        "# Pad images with inter nearest interpolation to size 40x40\n",
        "def pad_interpolation(input_pic):\n",
        "  res = cv2.resize(input_pic, dsize=(40, 40), interpolation=cv2.INTER_NEAREST)\n",
        "  return res"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jxHkoRPP73T"
      },
      "source": [
        "# Reorder frames as described in the paper \n",
        "def reorder_frames(action_input):\n",
        "  for f in range(action_input.shape[1]):\n",
        "    frame = action_input[:,f,:] # (20, 3)\n",
        "    idx = [0, 7, 9,11, 1, 8, 10, 12, 19, 2, 3, 6, 4, 13, 15, 17, 5, 14, 16, 18]\n",
        "    reordered_frame = frame[idx]\n",
        "    action_input[:,f,:] = reordered_frame\n",
        "  return action_input"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMWUoTgy8NvF"
      },
      "source": [
        "def clean_data(input):\n",
        "  zero_row = []\n",
        "  for i in range(input.shape[1]):\n",
        "    if (input[:,i] == np.zeros((20,3))).all():\n",
        "      zero_row.append(i)\n",
        "\n",
        "  new_seq = np.delete(input,zero_row,axis=1)\n",
        "  seq_len = new_seq.shape[0]\n",
        "\n",
        "  if np.any(new_seq) == False or seq_len < new_seq.shape[0]:\n",
        "    return None\n",
        "  \n",
        "  return new_seq"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPHTDDiXQBar"
      },
      "source": [
        "# Normalize all (x,y,z) coordinates as described in the paper\n",
        "def normalize(action_input):\n",
        "  for f in range(action_input.shape[1]):\n",
        "    frame = action_input[:,f,:] # (20, 3)\n",
        "    for j in range(len(frame)):\n",
        "      coords = frame[j,:]\n",
        "      normed_coords = normalize_coordinates(coords,frame, action_input)\n",
        "      action_input[j,f,:] = normed_coords\n",
        "  action_input=np.array(action_input, dtype=np.float32)\n",
        "  return action_input"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_lcfCdMP13-"
      },
      "source": [
        "# Normalize coordinates\n",
        "def normalize_coordinates(coords,frame, action):\n",
        "  min_value_x =  frame[:,0].min() \n",
        "  max_value_x = frame[:,0].max() \n",
        "  min_value_y =  frame[:,1].min() \n",
        "  max_value_y = frame[:,1].max() \n",
        "  min_value_z =  frame[:,2].min() \n",
        "  max_value_z = frame[:,2].max()\n",
        "  x_norm = (coords[0] - min_value_x) / (max_value_x - min_value_x)\n",
        "  y_norm = (coords[1] - min_value_y) / (max_value_y - min_value_y)\n",
        "  z_norm = (coords[2] - min_value_z) / (max_value_z - min_value_z)\n",
        "  return np.asarray([x_norm, y_norm, z_norm])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxKZcIB-QCOc"
      },
      "source": [
        "# Flip images horizontally and vertically\n",
        "def augment(data_x,data_y):\n",
        "  data_new_x = data_x\n",
        "  data_new_y = data_y\n",
        "  for idx, item in enumerate(data_x):\n",
        "    # flip\n",
        "    image_flipr = np.fliplr(item).reshape(1,32,32,3)\n",
        "    image_flipud = np.flipud(item).reshape(1,32,32,3)\n",
        "\n",
        "    data_new_x = np.concatenate((data_new_x, image_flipr))\n",
        "    data_new_x = np.concatenate((data_new_x, image_flipud))\n",
        "\n",
        "    data_new_y = np.concatenate((data_new_y, data_y[idx].reshape(1,8)))\n",
        "    data_new_y = np.concatenate((data_new_y, data_y[idx].reshape(1,8)))\n",
        "    \n",
        "  return data_new_x, data_new_y"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXzAwnUbgPlF"
      },
      "source": [
        "def equalize(action):\n",
        "  action=np.array(action, dtype=np.float32)\n",
        "  return exposure.equalize_adapthist(action, clip_limit=0.03)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8w_w40JQEO7"
      },
      "source": [
        "def read_and_preprocess_msr_data(path, group):\n",
        "  print(\"Reading in skeleton data...\")\n",
        "\n",
        "  labels, frame_sizes = [], []\n",
        "\n",
        "  group_train_X, group_train_Y = [], []\n",
        "  group_test_X, group_test_Y = [], []\n",
        "\n",
        "  # Get a list of all files the right order\n",
        "  file_names = sorted(listdir(path))\n",
        "  files = [join(path, file) for file in file_names]\n",
        "\n",
        "  for file in files:\n",
        "    # Get the label, the subject and the number of performance from filename\n",
        "    parts = file.split(\"/\").pop().split(\"_\")[:3]\n",
        "    print(parts)\n",
        "    label, subject, e = [int(x[1:].lstrip('0')) for x in parts]\n",
        "\n",
        "    # Get only first three values (for coordinates) and not the confidence score\n",
        "    action = np.loadtxt(file)[:, :3]\n",
        "\n",
        "    # Reshape\n",
        "    frame_size = len(action) // 20\n",
        "    frame_sizes.append(frame_size)\n",
        "    action = np.reshape(action, (20,frame_size, 3),order='F')\n",
        "\n",
        "    # Action cleaning\n",
        "    action = clean_data(action)\n",
        "    if action is None:\n",
        "      continue\n",
        "\n",
        "    # Reorder\n",
        "    action = reorder_frames(action)\n",
        "\n",
        "    # Resize and pad images\n",
        "    action = pad_interpolation(action)\n",
        "\n",
        "    # Normalize\n",
        "    action = normalize(action)\n",
        "\n",
        "    # Equalize\n",
        "    action = equalize(action)\n",
        "    \n",
        "    # Blur\n",
        "    #action = cv2.GaussianBlur(action, (3, 3), 0)\n",
        "\n",
        "    if subject in trainSubjects:\n",
        "      if label in group:\n",
        "        # We crop 8 times\n",
        "        for i in range(0,8):\n",
        "          cropped_action = action[i:i+32, i:i+32]\n",
        "          group_train_X.append(cropped_action)\n",
        "          group_train_Y.append(label)\n",
        "\n",
        "    elif subject in testSubjects:\n",
        "      if label in group:\n",
        "        # We crop 8 times\n",
        "        for i in range(0,8):\n",
        "          cropped_action = action[i:i+32, i:i+32]\n",
        "          group_test_X.append(cropped_action)\n",
        "          group_test_Y.append(label)\n",
        "\n",
        "  print('done')\n",
        "\n",
        "  # Normalize values to have less classes\n",
        "  uniques = list(set(group_train_Y))\n",
        "  for idx, item in enumerate(group_train_Y):\n",
        "    group_train_Y[idx] = uniques.index(item)\n",
        "\n",
        "  uniques = list(set(group_test_Y))\n",
        "  for idx, item in enumerate(group_test_Y):\n",
        "    group_test_Y[idx] = uniques.index(item)\n",
        "\n",
        "  # One-hot encode Y\n",
        "  group_train_Y = to_categorical(group_train_Y)\n",
        "  group_test_Y = to_categorical(group_test_Y)\n",
        "\n",
        "  # Make matrix out of X --> (#samples, 32, 32, 3)\n",
        "  group_train_X = np.stack(group_train_X,axis=0)\n",
        "  group_test_X = np.stack(group_test_X,axis=0)\n",
        "  \n",
        "  # Augment data to achieve better results\n",
        "  group_train_X, group_train_Y = augment(group_train_X, group_train_Y)\n",
        "  group_test_X, group_test_Y = augment(group_test_X, group_test_Y)\n",
        "\n",
        "  return group_train_X, group_train_Y, group_test_X, group_test_Y"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xc2yw-VkQK5s"
      },
      "source": [
        "x_train, y_train, x_test, y_test = read_and_preprocess_msr_data('/content/drive/MyDrive/folder/Skeleton20Joints_unrar/', groupAS3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iwi9FNExVMOo",
        "outputId": "02bd90c4-9640-488e-ec20-6f8fbf7a4b2b"
      },
      "source": [
        "print(f'x_train shape: {x_train.shape}, y_train shape: {y_train.shape}')\n",
        "print(f'x_test shape: {x_test.shape}, y_test shape: {y_test.shape}')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (2736, 32, 32, 3), y_train shape: (2736, 8)\n",
            "x_test shape: (2688, 32, 32, 3), y_test shape: (2688, 8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "BUKGuYyDVN7D",
        "outputId": "4000ff06-2520-4293-fe25-93cc5acbd5e9"
      },
      "source": [
        "imshow((x_test[0]))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fb3c8246ba8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAUpElEQVR4nO3dbYxcd3XH8e+Zp9312oX4gdQkAUOaUqWohGgVoCBEoaA0qhSQKgQvUKRGMqpIBRK8iKhaUqkvoCogXlS0polIK0qgPIioilrSCDVCKoFNahwnoU1IjYjr2M4DtuPs08ycvpjrahPdc3Y9Mztj8v99JMuz97//e8/8Z87M7tw995i7IyIvfY1pByAik6FkFymEkl2kEEp2kUIo2UUKoWQXKURrlMlmdi3wBaAJ/J27fzr7/k5rl891XlW/LyyZWX96cMmOxDO8eZ5721gjem30JPZGLxlqh2Ntj+dl8fdYTUaj/WV7TJ4iFs+L3kXWLF6rPI74fSl75kRjnsxy4rVPj+ZZjDPhWKM5X7u96d1wTitYx6WVE6x2T9cODp3sZtYE/hp4N/AE8CMzu9PdH47mzHVexW//+r/XjrU8DsUb9Xf6cPMPwznd7o5wLEuH7Om2rT8XTIofSGZOx/s79Ypw7JX9U+FY/BSAU/xv7fZ+P34C91kLx9x2x2PNeCV3BE/Gn7fitXLL4tgWjmVP4lZwv7vJrBU7m+wxSfZeHOMsl8dj82+q3f7y/slwzs5gHX/w8CfCOaP8GH8N8Ji7P+7uq8AdwPUj7E9EttAoyX4J8PN1Xz9RbRORC9CWf0BnZvvNbNHMFle7T2/14UQkMEqyHwUuW/f1pdW2F3D3A+6+4O4LndauEQ4nIqMYJdl/BFxhZq8xsw7wAeDO8YQlIuM29Kfx7t41s5uAf2Vw6u02d38om7N3e5M/fcv22rHsVacfbF+xv43jS/bXSwaTsyc01uo/iU3OQEE7ih5sJf5cfTY59ZZ9HP9kM5jXiz9F7ier1evHC9JoxPdtrlU/ttQJp9BP1rGfnvKKh5rBWHK38OT0YHKX0xgbxHe81a4/yzOzEp+daM/Up+4f/yw+CzXSeXZ3vwu4a5R9iMhk6C/oRAqhZBcphJJdpBBKdpFCKNlFCjHSp/Hnq9fs8eyuM7Vj0SkSAA/ObfXnktNaFo9l1VXRsQB8rf610frxsTqt+BTa8tJKOPZcL6mWW4uP91xQZbfaT6q8LH7N71lcmddoxGt1Njj11kgesz5xpWJ2WsuTqkPrB0/xJHZLxrLnaXrqsBsX1zQ69et/diU57dlcqt3eDdYd9M4uUgwlu0ghlOwihVCyixRCyS5SCCW7SCGU7CKFULKLFELJLlIIJbtIIZTsIoVQsosUYqKFMDbTp/nq5dqxXlZFEFwTbMd8fI2ulicFLdlY8vJnQQOUVlK0sjYXF5LsPVG/FgCd5XifzeR6cieDwo/luMYkLRp63uIYZ7Jr0AWdWGaSZitL2bXwkkvy9ZKGPL1grXrPxw/00tnZcKyRPE+bSRHVzpn4eGd/s744rBsUXgG0g+KZVvv8W3KJyEuMkl2kEEp2kUIo2UUKoWQXKYSSXaQQI516M7MjwBmgB3TdfWEcQYnI+I3jPPvvuPtTY9iPiGwh/RgvUohRk92B75rZ/Wa2fxwBicjWGPXH+Le5+1EzewVwt5n9xN3vXf8N1YvAfoBX7Lx0xMOJyLBGemd396PV/yeAbwPX1HzPAXdfcPeFl+3YOcrhRGQEQye7mc2b2Y5zt4H3AIfHFZiIjNcoP8ZfDHzbBhVpLeAf3f1f0hnm0KmvVEvbPzXqX5OaSSVRUkCVjsX1ZNC2+tKrTjcuKWu1uuHYfCdpG5VUeTWT9kRPB5Vo8714rVba8dOgE7STAmgkVW+dfv3xtp+J16O3mrQ7mo+PtRZ30QqrKdeStT/7VHyfdzTiZ0hWuLkjqUY7NVO/JnPE5Xz9uaAVWZISQye7uz8OvGHY+SIyWTr1JlIIJbtIIZTsIoVQsosUQskuUgglu0ghlOwihVCyixRCyS5SCCW7SCGU7CKFmGj7px1nG7zjB9vqB7MKlKBmodWKC1C8EY/1kte4taDoBoBup35//Tj4TmMpHHtmfl84FpdpMLjiX+C5bB2HOFjSWSktXopCfCarXUrisFPxWFKPE8bYSdZp/qJkf8mxsmSaSeZd/UD9YGcpLhqabdW3jJo/mxQTxSGIyEuJkl2kEEp2kUIo2UUKoWQXKYSSXaQQSnaRQijZRQqhZBcphJJdpBBKdpFCKNlFCqFkFynEhlVvZnYb8PvACXd/fbVtJ/A1YB9wBHi/uz+74b7aMHtZWs9VL6quasZVQVkZXVat1cpe/9bqW1d5Lz5Wey6+v82kfK2ZxOFJmdrzy/WxNNfitVpdXQ7HeqtxjMsW3++XLc/Wz/H4KddNqvmykjjrx2NRp6ysTVIjafHUTOJoWfzEmm+shmN9q1//uW3xsXxPMJC0mdrMO/uXgWtftO1m4B53vwK4p/paRC5gGyZ71W/9mRdtvh64vbp9O/DeMcclImM27O/sF7v7ser2kww6uorIBWzkD+jc3UmaIJvZfjNbNLPFk2efHvVwIjKkYZP9uJntBaj+PxF9o7sfcPcFd1/YM79ryMOJyKiGTfY7gRuq2zcA3xlPOCKyVTZMdjP7KvAfwOvM7AkzuxH4NPBuM3sU+N3qaxG5gG14nt3dPxgMvWvMsYjIFtJf0IkUQskuUgglu0ghlOwihVCyixRior3evOOsXRpX/0QsqGB7vh2XSZnHlUtJ0RuWvP41Vuorxyzp9bY6E8e41I0r0ZKCJ7pJ5dW+n9R3Z2uttMM5vX59DzsAVuLHa7VbXwUI0Fyub872VFKouNKPH5meZxWOSUVcsMtm8jbXSqr5OkljudlWHP9cK17HmU591Vs/ee40frX+8Wy043XSO7tIIZTsIoVQsosUQskuUgglu0ghlOwihVCyixRCyS5SCCW7SCGU7CKFULKLFELJLlKIyRbCNGB5pr5YwJLig15QINHrxK9Vsx6PdZPCiaR+hrVgXqebFHDMxjtcS1oyeVIU0k/i76wERS1L4RQaydqzEq9j25OSotX6+9Zai+d0e1lrsLgopJ+10YoKYZrxkVpJ+6d21E8K6CSPy0zS/oneSu3mRlJ0w0wQY9KCSu/sIoVQsosUQskuUgglu0ghlOwihVCyixRiM+2fbjOzE2Z2eN22W8zsqJkdrP5dt7VhisioNvPO/mXg2prtn3f3q6p/d403LBEZtw2T3d3vBZ6ZQCwisoVG+Z39JjM7VP2Yf9HYIhKRLTFssn8RuBy4CjgGfDb6RjPbb2aLZrb41OmnhzyciIxqqGR39+Pu3nP3PvAl4Jrkew+4+4K7L+z+lV3DxikiIxoq2c1s77ov3wccjr5XRC4MG1a9mdlXgXcAu83sCeBTwDvM7CoGnZSOAB/ezMG6J+HUrVG1TjIxKORZTirDrJVUDCXVWv2kqsm69a+NUXsqgKbH5VXNVtx26XRSvWRxkRdPNuqrzfoet2pqJOuxnLS2ej55r1hr1rehWmrF67GWtn9Kqt6SdljR0SypUKOXPGb9eF5rJWkNlVTm0d1Zu9n+J56zbbH+cVl5Ok7pDZPd3T9Ys/nWjeaJyIVFf0EnUgglu0ghlOwihVCyixRCyS5SCCW7SCGU7CKFULKLFELJLlIIJbtIIZTsIoVQsosUYqK93ugY/sr6aihLmqxFtVDNdtzAzJJeb0mHMjx5/Wv36pfLk9j7reVwzJqzWSDJWFKxFfUHS+YkxVop7ydPHwvWqhHPaSZxNJOqN0uq9qL6taSYL+3310wq8zx50Poe9/XrWP0d37ZW3wMOoDcTjB0Mp+idXaQUSnaRQijZRQqhZBcphJJdpBBKdpFCKNlFCqFkFymEkl2kEEp2kUIo2UUKoWQXKcRm2j9dBvw9cDGD8owD7v4FM9sJfA3Yx6AF1Pvd/dn0YO0Wu145vuaOTyTFBb121koo3mfQ4QkAX2vXbk9qI5hNejUdb2Q9r1bjOLpJ4U3Q1qibFJJ0k/j7yVNkNWmVRSt4bGbi1kreSxa/X7/2g7F4KOyilb3NJWNRnRFAthzbkrE9M/WPjZ/dHs65KCieabTjddrMO3sX+Li7Xwm8GfiImV0J3Azc4+5XAPdUX4vIBWrDZHf3Y+7+QHX7DPAIcAlwPXB79W23A+/dqiBFZHTn9Tu7me0D3gjcB1zs7seqoScZ/JgvIheoTSe7mW0Hvgl8zN1Prx9zdye43IKZ7TezRTNbfOr5kyMFKyLD21Sym1mbQaJ/xd2/VW0+bmZ7q/G9wIm6ue5+wN0X3H1h97Y944hZRIawYbKbmTHox/6Iu39u3dCdwA3V7RuA74w/PBEZl81cg+6twIeAB83s3BWuPgl8Gvi6md0I/Ax4/9aEKCLjsGGyu/v3gejE7rvGG46IbBX9BZ1IIZTsIoVQsosUQskuUgglu0ghJtr+ydrQvOT850WtejxoMQR5BVLWWWmYOrSsim7O4iqvRlIu189aSnm8z7Wgyq6f3euggmogntfqJ1WHjbX67cQtjfphsyZoJGPZYxZGnxXYJe2ksvWwJJKux9Vov7D6lmi/sjuJolV/B7wTx6B3dpFCKNlFCqFkFymEkl2kEEp2kUIo2UUKoWQXKYSSXaQQSnaRQijZRQqhZBcphJJdpBATLYRZBh6NuxCFotKDZ1txi6RW0hMoq3PwZF6PoPAjKWh5xmbDsZneqXBsWyNeqLWspiWIsd+P92fJgixlrZCy7k+9+nVcaWatq+KDrWXlLkNUNmUtu9IdJmuVlRP1k6Kts436QhjvxsUzKz5fu301iU/v7CKFULKLFELJLlIIJbtIIZTsIoVQsosUYjO93i4zs++Z2cNm9pCZfbTafouZHTWzg9W/67Y+XBEZ1mbOs3eBj7v7A2a2A7jfzO6uxj7v7n+1deGJyLhsptfbMeBYdfuMmT0CDHGNWBGZpvP6nd3M9gFvBO6rNt1kZofM7DYzu2jMsYnIGG062c1sO/BN4GPufhr4InA5cBWDd/7PBvP2m9mimS0++9zJMYQsIsPYVLKbWZtBon/F3b8F4O7H3b3n7n3gS8A1dXPd/YC7L7j7wkXb94wrbhE5T5v5NN6AW4FH3P1z67bvXfdt7wMOjz88ERmXzXwa/1bgQ8CDZnaw2vZJ4INmdhWDEqEjwIc32tHsvPO6Ny3XjmUVQ1Hl0orFlVDJUFr11s+qq/qdYH/JDpMqL1vZkUxL4qjvrARAI4ilT1L1llUIxocK23IB9Br1+3SL3188WXtL2mHhcfxR06i06C2TVRwmz4OsNVSjEUQZtHgC8Jn6Y81tD6ds6tP471OfbndtNFdELhz6CzqRQijZRQqhZBcphJJdpBBKdpFCKNlFCqFkFymEkl2kEEp2kUIo2UUKoWQXKYSSXaQQE+311pszzry+vnIsqWkKB5ezZmOJfl7LFY54VOaV7K7RjJe4v5JUSWX7DFrOAfR7weu3x6/rljQ+y/qvefZe0arfp4d1aGBZFWP2kA0xNnSvtySQ7OkYPncAWsFYkp2t+fo5/bmkui7enYi8lCjZRQqhZBcphJJdpBBKdpFCKNlFCqFkFymEkl2kEEp2kUIo2UUKoWQXKYSSXaQQGxbCmNkscC8wU33/N9z9U2b2GuAOYBdwP/Ahd19ND9bosXP+zOhR/7/2cNPSXlNJNUM3KOLIWhO1kyVZjYtC0tfhrDVUNJbe5yEN81aRzcmqoYbt1zTUvKzVVDItW+Ms06KncTZnNthVEvpmHq4V4J3u/gYG7ZmvNbM3A58BPu/uvwY8C9y4iX2JyJRsmOw+8Fz1Zbv658A7gW9U228H3rslEYrIWGy2P3uz6uB6Argb+CnwC3c/V1n9BHDJ1oQoIuOwqWR39567XwVcClwD/MZmD2Bm+81s0cwWT558esgwRWRU5/URi7v/Avge8Bbg5WZ27iOES4GjwZwD7r7g7gt79uwaKVgRGd6GyW5me8zs5dXtOeDdwCMMkv4Pqm+7AfjOVgUpIqPbzDXo9gK3m1mTwYvD1939n83sYeAOM/sL4D+BW7cwThEZ0YbJ7u6HgDfWbH+cwe/vIvJLQH9BJ1IIJbtIIZTsIoVQsosUQskuUghzH7acaIiDmZ0EflZ9uRt4amIHjymOF1IcL/TLFser3X1P3cBEk/0FBzZbdPeFqRxccSiOAuPQj/EihVCyixRimsl+YIrHXk9xvJDieKGXTBxT+51dRCZLP8aLFGIqyW5m15rZf5nZY2Z28zRiqOI4YmYPmtlBM1uc4HFvM7MTZnZ43badZna3mT1a/X/RlOK4xcyOVmty0Myum0Acl5nZ98zsYTN7yMw+Wm2f6JokcUx0Tcxs1sx+aGY/ruL482r7a8zsvipvvmZmnfPasbtP9B/QZHBZq9cCHeDHwJWTjqOK5QiwewrHfTtwNXB43ba/BG6ubt8MfGZKcdwCfGLC67EXuLq6vQP4b+DKSa9JEsdE14TB5W23V7fbwH3Am4GvAx+otv8N8Efns99pvLNfAzzm7o/74NLTdwDXTyGOqXH3e4FnXrT5egYX7oQJXcAziGPi3P2Yuz9Q3T7D4OIolzDhNUnimCgfGPtFXqeR7JcAP1/39TQvVunAd83sfjPbP6UYzrnY3Y9Vt58ELp5iLDeZ2aHqx/wt/3ViPTPbx+D6CfcxxTV5URww4TXZiou8lv4B3dvc/Wrg94CPmNnbpx0QDF7ZGb4twqi+CFzOoEfAMeCzkzqwmW0Hvgl8zN1Prx+b5JrUxDHxNfERLvIamUayHwUuW/d1eLHKrebuR6v/TwDfZrpX3jluZnsBqv9PTCMIdz9ePdH6wJeY0JqYWZtBgn3F3b9VbZ74mtTFMa01qY593hd5jUwj2X8EXFF9stgBPgDcOekgzGzezHacuw28Bzicz9pSdzK4cCdM8QKe55Kr8j4msCZmZgyuYfiIu39u3dBE1ySKY9JrsmUXeZ3UJ4wv+rTxOgafdP4U+JMpxfBaBmcCfgw8NMk4gK8y+HFwjcHvXjcy6Jl3D/Ao8G/AzinF8Q/Ag8AhBsm2dwJxvI3Bj+iHgIPVv+smvSZJHBNdE+C3GFzE9RCDF5Y/W/ec/SHwGPBPwMz57Fd/QSdSiNI/oBMphpJdpBBKdpFCKNlFCqFkFymEkl2kEEp2kUIo2UUK8X9HedXJa3bLnQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCl3pms8kwVc"
      },
      "source": [
        "### ResNet Keras\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPUOc6YYk96b",
        "outputId": "029d6184-38b4-411a-8447-498e8a461225"
      },
      "source": [
        "shape, classes = (32,32, 3), 8\n",
        "\n",
        "x = keras.layers.Input(shape)\n",
        "\n",
        "model = tf.keras.applications.ResNet50(input_tensor=x, classes = 8, weights = None)\n",
        "\n",
        "model.compile(optimizer='adam', loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "#model.summary()\n",
        "for i in range(10):\n",
        "  model.fit(x_train, y_train, epochs=100, batch_size=32, verbose=0)\n",
        "  _, accuracy = model.evaluate(x_test, y_test, verbose=1)\n",
        "  print(f'Score: {accuracy*100.0}')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "84/84 [==============================] - 1s 9ms/step - loss: 1.2550 - accuracy: 0.8415\n",
            "Score: 84.15178656578064\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 1.2611 - accuracy: 0.8032\n",
            "Score: 80.31994104385376\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 1.3928 - accuracy: 0.8583\n",
            "Score: 85.82589030265808\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 1.9073 - accuracy: 0.7347\n",
            "Score: 73.4747052192688\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 2.4941 - accuracy: 0.7816\n",
            "Score: 78.1622052192688\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 1.1857 - accuracy: 0.8251\n",
            "Score: 82.51488208770752\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 1.0578 - accuracy: 0.8839\n",
            "Score: 88.39285969734192\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 1.6102 - accuracy: 0.8493\n",
            "Score: 84.93303656578064\n",
            "84/84 [==============================] - 1s 9ms/step - loss: 2.0130 - accuracy: 0.8579\n",
            "Score: 85.78869104385376\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RexFyD4pM1ef"
      },
      "source": [
        "## ResNet with pretrained weights "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "617bnD9nKg_r"
      },
      "source": [
        "shape, classes = (32,32, 3), 8\n",
        "\n",
        "x = keras.layers.Input(shape)\n",
        "\n",
        "pretrained_model = tf.keras.applications.ResNet50(input_tensor=x,weights='imagenet', include_top=False)\n",
        "\n",
        "\n",
        "x = pretrained_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dropout(0.7)(x)\n",
        "predictions = Dense(classes, activation= 'softmax')(x)\n",
        "model = Model(inputs = pretrained_model.input, outputs = predictions)\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam', loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "for i in range(10):\n",
        "  model.fit(x_train, y_train, epochs=100, batch_size=32, verbose=0)\n",
        "  _, accuracy = model.evaluate(x_test, y_test, verbose=1)\n",
        "  print(f'Score: {accuracy*100.0}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FIbeme_UW3E"
      },
      "source": [
        "## ResNet Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SL6TfOSAdr5p"
      },
      "source": [
        "def lr_schedule(epoch):\n",
        "  lr = 0.01\n",
        "  if epoch > 74:\n",
        "    lr = 0.001\n",
        "  if epoch > 149:\n",
        "    lr = 0.0001\n",
        "  print('Learning rate: ', lr)\n",
        "  return lr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUuK3nx1QQYk"
      },
      "source": [
        "def resnet_layer(inputs, num_filters=16, kernel_size=3, strides=1, dropout=False, first = False):\n",
        "  \n",
        "  conv = Conv2D(num_filters, kernel_size=kernel_size, strides=strides, padding='same')\n",
        "\n",
        "  x = inputs\n",
        "\n",
        "  if first:\n",
        "    x = conv(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "  else:\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    if dropout:\n",
        "      x = Dropout(rate=0.25)(x)\n",
        "    x = conv(x)\n",
        "\n",
        "  return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oZR9YHqQZfB"
      },
      "source": [
        "def resnet20(num_classes=8):\n",
        "    \n",
        "    num_filters = 16\n",
        "    num_res_blocks = 3\n",
        "    shape = (32,32, 3)\n",
        "    \n",
        "\n",
        "    inputs = Input(shape)\n",
        "    x = resnet_layer(inputs=inputs, first=True)\n",
        "\n",
        "    # Instantiate the stack of residual units\n",
        "    for stack in range(3): \n",
        "        for res_block in range(num_res_blocks): # 3x 16, 3x 32, 3x 64\n",
        "            strides = 1\n",
        "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
        "                strides = 2  # downsample\n",
        "            y = resnet_layer(inputs=x,num_filters=num_filters,strides=strides)\n",
        "            y = resnet_layer(inputs=y,num_filters=num_filters,dropout=True)\n",
        "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
        "                # linear projection residual shortcut connection to match\n",
        "                # changed dims\n",
        "                x = resnet_layer(inputs=x,num_filters=num_filters,kernel_size=1,strides=strides)\n",
        "            x = keras.layers.add([x, y])\n",
        "        num_filters *= 2\n",
        "\n",
        "    # Add classifier on top.\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    y = Flatten()(x)\n",
        "    outputs = Dense(num_classes, activation='softmax')(y)\n",
        "\n",
        "    # Instantiate model\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6WSflZSetc0"
      },
      "source": [
        "lr_scheduler = LearningRateScheduler(lr_schedule)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuWvQoweQcAL"
      },
      "source": [
        "model = resnet20(8)\n",
        "model.compile(loss='categorical_crossentropy',optimizer=SGD(lr=lr_schedule(0),momentum=0.9, decay=0.0001),metrics=['accuracy'])\n",
        "#model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGc59FU6QmkH"
      },
      "source": [
        "#model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utEGvxp_QhEO"
      },
      "source": [
        "model.fit(x_train, y_train,batch_size=batch_size,epochs=epochs,validation_data=(x_test, y_test), callbacks=[lr_scheduler])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBqBQjRGnUdd"
      },
      "source": [
        "base_model = ResNet50(weights = 'imagenet', include_top = False, input_shape = (224,224,3))\n",
        "\n",
        "\n",
        "head = base_model.output\n",
        "head = Flatten()(head)\n",
        "head = Dropout(0.5)(head)\n",
        "head = Dense(4096,activation='relu')(head)\n",
        "head = Dropout(0.5)(head)\n",
        "head = Dense(2048,activation='relu')(head)\n",
        "head = Dropout(0.5)(head)\n",
        "head = Dense(60,activation='softmax')(head)\n",
        "model = Model(inputs = base_model.input, outputs = head)\n",
        "\n",
        "#VGG16, VGG19"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}